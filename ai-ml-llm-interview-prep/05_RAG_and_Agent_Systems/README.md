# 05 RAG and Agent Systems

## Overview

**Retrieval-Augmented Generation (RAG)** and **Agent Systems** represent the cutting edge of LLM applications. This section covers how to build production-ready systems that combine LLMs with external knowledge and tools.

**Why This Matters:**
- RAG is the #1 enterprise LLM use case
- Agents are the future of LLM applications
- These topics appear in almost every modern AI interview

---

## üìö Contents

### RAG Fundamentals

1. [**RAG Architecture Overview**](./rag_fundamentals.md) ‚≠ê‚≠ê‚≠ê
   - What is RAG and why use it?
   - RAG vs Fine-tuning vs Prompting
   - Basic RAG pipeline
   - Advanced RAG patterns

2. [**Vector Databases**](./vector_databases.md) ‚≠ê‚≠ê‚≠ê
   - Why vector databases?
   - Popular options (Pinecone, Weaviate, Qdrant, ChromaDB, FAISS)
   - ANN algorithms (HNSW, IVF)
   - Choosing the right vector DB

3. [**Embeddings for RAG**](./embeddings_for_rag.md) ‚≠ê‚≠ê‚≠ê
   - Choosing embedding models
   - Sentence transformers
   - OpenAI embeddings vs open-source
   - Embedding dimensionality tradeoffs
   - Similarity metrics (cosine vs dot product)

4. [**Chunking Strategies**](./chunking_strategies.md) ‚≠ê‚≠ê‚≠ê
   - Fixed-size vs semantic chunking
   - Chunk size optimization
   - Overlap strategies
   - Document structure preservation

5. [**Retrieval Optimization**](./retrieval_optimization.md) ‚≠ê‚≠ê
   - Hybrid search (dense + sparse)
   - Query expansion
   - Metadata filtering
   - Multi-stage retrieval

6. [**Reranking**](./reranking.md) ‚≠ê‚≠ê
   - Why rerank?
   - Cross-encoders vs bi-encoders
   - Reranking models (Cohere, bge-reranker)
   - Cost-accuracy tradeoffs

7. [**Evaluation for RAG**](./rag_evaluation.md) ‚≠ê‚≠ê‚≠ê
   - Retrieval metrics (Recall@K, MRR, NDCG)
   - Generation metrics (Faithfulness, Relevance)
   - End-to-end evaluation
   - RAGAs framework

### Advanced RAG

8. [**Graph RAG**](./graph_rag.md) ‚≠ê‚≠ê
   - When documents have relationships
   - Knowledge graph construction
   - Graph-based retrieval

9. [**Multi-Hop Retrieval**](./multi_hop_retrieval.md) ‚≠ê
   - Complex queries needing multiple retrievals
   - Iterative retrieval patterns

10. [**RAG Failure Modes and Debugging**](./rag_debugging.md) ‚≠ê‚≠ê
    - Common failure patterns
    - How to diagnose issues
    - Mitigation strategies

### Agent Systems

11. [**Agent Architectures**](./agent_architectures.md) ‚≠ê‚≠ê‚≠ê
    - What is an agent?
    - ReAct (Reasoning + Acting)
    - Plan-and-Execute
    - Reflection and self-correction
    - Multi-agent systems

12. [**Tool Use and Function Calling**](./tool_use.md) ‚≠ê‚≠ê
    - How agents use tools
    - Tool selection strategies
    - Error handling

13. [**Memory Systems**](./memory_systems.md) ‚≠ê‚≠ê
    - Short-term vs long-term memory
    - Conversation history management
    - Entity memory

14. [**Agent Orchestration**](./agent_orchestration.md) ‚≠ê‚≠ê
    - LangChain, LlamaIndex, AutoGPT
    - When to use frameworks vs custom
    - Production considerations

### Production RAG/Agents

15. [**Production RAG Systems**](./production_rag.md) ‚≠ê‚≠ê‚≠ê
    - Architecture patterns
    - Latency optimization
    - Cost optimization
    - Monitoring and observability
    - Scaling strategies

16. [**RAG Security**](./rag_security.md) ‚≠ê
    - Prompt injection via retrieved docs
    - Data poisoning
    - Access control

---

## üéØ Learning Objectives

After this section, you should be able to:

- Design and implement a production RAG system
- Choose appropriate embedding models and vector databases
- Optimize retrieval quality and latency
- Evaluate RAG systems effectively
- Build agent systems with tool use
- Debug common RAG failure modes
- Scale RAG systems for production traffic

---

## ‚è±Ô∏è Time Estimate

**Total: 10-12 hours**

**High Priority (Interview Critical):**
- RAG Fundamentals: 1.5 hours ‚≠ê‚≠ê‚≠ê
- Vector Databases: 1 hour ‚≠ê‚≠ê‚≠ê
- Embeddings for RAG: 1.5 hours ‚≠ê‚≠ê‚≠ê
- Chunking Strategies: 1 hour ‚≠ê‚≠ê‚≠ê
- RAG Evaluation: 1 hour ‚≠ê‚≠ê‚≠ê
- Agent Architectures: 1.5 hours ‚≠ê‚≠ê‚≠ê
- Production RAG: 1.5 hours ‚≠ê‚≠ê‚≠ê

**Medium Priority:**
- Retrieval Optimization: 1 hour ‚≠ê‚≠ê
- Reranking: 45 min ‚≠ê‚≠ê
- Tool Use: 45 min ‚≠ê‚≠ê
- RAG Debugging: 30 min ‚≠ê‚≠ê

**Lower Priority:**
- Graph RAG: 30 min ‚≠ê
- Multi-Hop Retrieval: 30 min ‚≠ê
- RAG Security: 20 min ‚≠ê

---

## üî• Interview Focus Areas

### Most Common Interview Questions

1. **"Design a RAG system for [company knowledge base / customer support / etc.]"**
   - See: [RAG Fundamentals](./rag_fundamentals.md), [Production RAG](./production_rag.md)

2. **"How do you choose chunk size?"**
   - See: [Chunking Strategies](./chunking_strategies.md)

3. **"What's the difference between RAG and fine-tuning?"**
   ```
   RAG:
   ‚úÖ Up-to-date information (just update docs)
   ‚úÖ Interpretable (can show sources)
   ‚úÖ Cost-effective
   ‚ùå Retrieval quality dependency
   ‚ùå Added latency

   Fine-tuning:
   ‚úÖ Knowledge baked into model
   ‚úÖ Faster inference (no retrieval)
   ‚úÖ Learns style and patterns
   ‚ùå Static knowledge (need retrain for updates)
   ‚ùå Expensive
   ‚ùå Less interpretable

   Often best: Hybrid (fine-tune for style, RAG for facts)
   ```

4. **"How do you evaluate RAG quality?"**
   - Retrieval: Recall@K, MRR, NDCG
   - Generation: Faithfulness, Answer Relevance
   - End-to-end: Human eval, LLM-as-judge
   - See: [RAG Evaluation](./rag_evaluation.md)

5. **"What causes poor RAG performance?"**
   - Bad retrieval (wrong chunks returned)
   - Poor chunking (split in wrong places)
   - Embedding model mismatch (query vs doc embeddings)
   - Context window limitations
   - See: [RAG Debugging](./rag_debugging.md)

6. **"Explain agent architectures"**
   - ReAct: Reason about what to do, Act (use tool), Observe result, repeat
   - Plan-and-Execute: Plan steps upfront, execute sequentially
   - See: [Agent Architectures](./agent_architectures.md)

---

## üöÄ Quick Start Paths

### Path 1: RAG-Focused (6-7 hours)

**For roles emphasizing RAG systems:**

1. [RAG Fundamentals](./rag_fundamentals.md) - 1.5 hours
2. [Embeddings for RAG](./embeddings_for_rag.md) - 1.5 hours
3. [Vector Databases](./vector_databases.md) - 1 hour
4. [Chunking Strategies](./chunking_strategies.md) - 1 hour
5. [RAG Evaluation](./rag_evaluation.md) - 1 hour
6. [Production RAG](./production_rag.md) - 1.5 hours

### Path 2: Agent-Focused (4-5 hours)

**For roles emphasizing autonomous systems:**

1. [Agent Architectures](./agent_architectures.md) - 1.5 hours
2. [Tool Use](./tool_use.md) - 45 min
3. [Memory Systems](./memory_systems.md) - 1 hour
4. [Agent Orchestration](./agent_orchestration.md) - 1 hour
5. Review agent case studies - 30-45 min

### Path 3: Complete Coverage (10-12 hours)

**Week 1: RAG Fundamentals**
- Days 1-2: RAG basics, embeddings, vector DBs
- Days 3-4: Chunking, retrieval, reranking
- Day 5: RAG evaluation

**Week 2: Advanced Topics**
- Days 1-2: Agent architectures and tool use
- Days 3-4: Production RAG systems
- Day 5: Practice with case studies

---

## üéì Key Concepts to Master

### 1. Basic RAG Pipeline

```mermaid
graph LR
    A[User Query] --> B[Embed Query]
    B --> C[Vector Search]
    D[Document Store] --> E[Chunked & Embedded]
    E --> C
    C --> F[Top-K Chunks]
    F --> G[Rerank Optional]
    G --> H[Build Context]
    H --> I[LLM Prompt]
    I --> J[Generate Answer]
    J --> K[Response with Citations]
```

### 2. RAG vs Fine-Tuning Decision Matrix

| Consideration | Use RAG | Use Fine-Tuning | Use Both |
|--------------|---------|-----------------|----------|
| **Need up-to-date info** | ‚úÖ | ‚ùå | ‚úÖ |
| **Knowledge is static** | Either | ‚úÖ | Either |
| **Need citations** | ‚úÖ | ‚ùå | ‚úÖ |
| **Have labeled data** | Not needed | Required ‚úÖ | ‚úÖ |
| **Need specific style/tone** | Prompting may suffice | ‚úÖ | ‚úÖ |
| **Budget limited** | ‚úÖ (cheaper) | ‚ùå | Depends |
| **Latency critical** | ‚ùå (adds latency) | ‚úÖ | ‚ùå |
| **Interpretability needed** | ‚úÖ (show sources) | ‚ùå | ‚úÖ |

### 3. Chunking Strategies Comparison

| Strategy | Pros | Cons | When to Use |
|----------|------|------|-------------|
| **Fixed-size (512 tokens)** | Simple, predictable | Might split mid-sentence | Quick start, simple docs |
| **Sentence-based** | Semantic integrity | Variable size | General purpose |
| **Paragraph-based** | Natural boundaries | Might be too large | Structured documents |
| **Semantic chunking** | Coherent topics | More complex | High-quality docs |
| **Sliding window (overlap)** | No info loss at boundaries | Redundancy | Critical context preservation |

### 4. Embedding Model Selection

| Model | Dimensions | Speed | Quality | Cost | Use Case |
|-------|-----------|-------|---------|------|----------|
| **OpenAI text-embedding-3-small** | 1536 | Fast | Great | $$$ | Production, high quality |
| **sentence-transformers/all-MiniLM-L6-v2** | 384 | Very Fast | Good | Free (self-host) | Cost-sensitive, fast retrieval |
| **bge-large-en-v1.5** | 1024 | Medium | Excellent | Free | Open-source, quality |
| **OpenAI text-embedding-3-large** | 3072 | Medium | Best | $$$$ | Maximum quality |

### 5. Vector Database Selection

| Database | Hosting | Scale | Speed | Features | Best For |
|----------|---------|-------|-------|----------|----------|
| **FAISS** | Self (in-memory) | Medium | Fastest | Minimal | Prototyping, small scale |
| **ChromaDB** | Self or cloud | Small-Medium | Fast | Simple API | Development, small prod |
| **Pinecone** | Cloud-only | Large | Fast | Managed, features | Production, no ops overhead |
| **Weaviate** | Both | Large | Fast | GraphQL, hybrid search | Complex queries |
| **Qdrant** | Both | Large | Fast | Filtering, payloads | Production, self-hosted |
| **pgvector** | Self (Postgres) | Medium | Medium | Familiar SQL | Existing Postgres users |

---

## üí° Pro Tips for Interviews

### 1. Always Start with the Full Picture

**Bad:** "I'll use FAISS for vector search."

**Good:**
"Let me outline the RAG architecture:
1. **Ingestion**: Chunk documents (512 tokens, 50 overlap), embed with Sentence-BERT, store in vector DB
2. **Retrieval**: Embed query, vector search (top-10), rerank (top-3)
3. **Generation**: Build prompt with context, call LLM, return answer with citations

For vector DB, I'd start with ChromaDB for development (simple) and move to Pinecone for production (managed, scalable). I'd use bge-large for embeddings (good quality, free)."

### 2. Discuss Failure Modes Proactively

"Key risks with RAG:
- **Poor retrieval**: Wrong chunks returned ‚Üí Fix with better embeddings, hybrid search, metadata filtering
- **Chunk boundaries**: Split context ‚Üí Fix with overlap, better chunking
- **Hallucination**: LLM makes up info ‚Üí Fix with stricter prompts, citations required, lower temperature
- **Latency**: Retrieval + generation slow ‚Üí Fix with caching, parallel retrieval, faster embeddings"

### 3. Show You Think About Production

"For production:
- **Latency**: Target <2s end-to-end. Cache frequent queries, use fast embeddings, consider async where possible
- **Cost**: Embedding costs (e.g., $0.0001 per 1K tokens for OpenAI) √ó query volume. For 1M queries/month with avg 1K tokens ‚Üí $100/month. LLM calls dominate cost.
- **Monitoring**: Track retrieval precision@K, answer relevance, latency p50/p99, cost per query
- **Evaluation**: Maintain eval set (100+ query-answer pairs), run before deploys"

### 4. Compare Approaches

**Interviewer:** "Should we use RAG or fine-tuning?"

**Your answer:**
"Let me think through the tradeoffs:

**Our requirements:**
- Customer support knowledge base (updated weekly)
- Need to cite sources for compliance
- Budget: $5K/month

**Analysis:**
- Fine-tuning: $2K one-time + $2K/week retraining = $10K/month, no citations
- RAG: $200/month vector DB + $1K/month LLM calls = $1.2K/month, built-in citations, instant updates

**Recommendation:** RAG is better here. We get citations (required), lower cost, and easy updates. If we later find style/tone issues, we can add instruction fine-tuning on top of RAG (hybrid approach)."

---

## üîó Connections to Other Sections

- **03 Modern NLP & Transformers**: Embeddings deep dive
- **04 Large Language Models**: LLMs used in generation phase
- **06 MLOps**: Deploying and monitoring RAG systems
- **07 System Design**: RAG system architecture patterns
- **08 Case Studies**: Real RAG applications (customer support, knowledge base)

---

## üìà Success Metrics

You've mastered this section when you can:

- [ ] Design a complete RAG system architecture
- [ ] Choose appropriate chunking strategy and explain why
- [ ] Select embedding models and vector DBs with justification
- [ ] Implement retrieval optimization (hybrid search, reranking)
- [ ] Evaluate RAG systems using proper metrics
- [ ] Debug common RAG failure modes
- [ ] Explain agent architectures (ReAct, Plan-Execute)
- [ ] Discuss production considerations (latency, cost, monitoring)

---

## üéØ Common Mistakes to Avoid

‚ùå "RAG is always better than fine-tuning" ‚Üí Each has use cases, often best is hybrid
‚ùå "Bigger chunks are better" ‚Üí Too big loses precision, too small loses context
‚ùå "Top-1 retrieval is enough" ‚Üí Usually need top-3 to top-10, then rerank
‚ùå "Vector search is sufficient" ‚Üí Hybrid search (dense + sparse) often better
‚ùå "Any embedding model works" ‚Üí Quality varies significantly, domain matters
‚ùå "No need to evaluate retrieval separately" ‚Üí Retrieval quality determines system ceiling

---

**Ready to start?** ‚Üí [RAG Fundamentals](./rag_fundamentals.md) (Most interview-critical)

**Next Section:** [06 MLOps and Production AI](../06_MLOps_and_Production_AI/README.md)
