# Case Study: LLM Cost Optimization

**Scenario:** You deployed a highly successful GenAI summarizing tool for a company's internal logistics reports. It uses GPT-4o and processes 100,000 documents a day. The product managers love it, but the CFO just saw the AWS/OpenAI bill: It's costing \$50,000 a month. You have been tasked with reducing the inference cost by 80% within 4 weeks without significantly degrading the quality of the summaries.

---

## 1. Problem Framing
*   **Current State:** $100k~docs/day 	imes 5k~input~tokens 	imes \$0.005/1k~tokens = \$2,500/day$.
*   **Goal:** Reduce to $<\$500/day$.
*   **Constraint:** Maintain readability and core fact extraction accuracy.

## 2. Strategy 1: Prompt & Context Optimization (The Easiest Fix)
The easiest way to reduce token costs is to send fewer tokens.
*   **The Problem:** The current system simply dumps the entire 10-page logistics PDF into the prompt.
*   **The Fix:** 
    *   Implement **Heuristic Truncation:** We discover that 90% of the valuable summary information is always in the "Executive Summary" and "Conclusion" sections of the PDFs. We write a Python script to strip out the middle 6 pages of raw data tables before sending the text to the LLM. 
    *   *Result:* Input tokens cut by 50%. Cost drops by 50%.

## 3. Strategy 2: Model Cascading (The Router Pattern)
Not all documents require the world's smartest AI.
*   **The Problem:** We are using GPT-4o to summarize simple, 1-paragraph status update emails, which is massive overkill.
*   **The Fix:** Implement a **Size-Based Router**.
    *   If `document_length < 500 words`, send it to a cheap, fast model like `GPT-4o-mini` or `Claude 3 Haiku` (Cost: \$0.15 / 1M tokens).
    *   If `document_length >= 500 words`, or if it contains complex tables, route to `GPT-4o` (Cost: \$5.00 / 1M tokens).
    *   *Result:* 60% of traffic is diverted to a model that is 30x cheaper.

## 4. Strategy 3: The Batch API (Asynchronous Execution)
Does the user need the summary in 3 seconds, or can they wait?
*   **The Problem:** We are hitting the synchronous REST API for every document immediately.
*   **The Fix:** For historical document processing (backlogs) or nightly reports, we use OpenAI's **Batch API**. You upload a massive JSONL file of thousands of requests, and OpenAI processes them within 24 hours.
*   *Result:* The Batch API offers a strict **50% discount** on all token costs.

## 5. Strategy 4: Fine-Tuning a Smaller Open-Source Model (The Ultimate Fix)
This requires engineering effort but yields the highest long-term ROI.
*   **The Plan:** We don't need a model that knows how to write Python or speak French. We just need a model that summarizes logistics reports.
*   **Data Generation:** We take 10,000 examples of the *excellent* summaries generated by GPT-4o over the last month. This becomes our "Golden Dataset".
*   **Knowledge Distillation (SFT):** We use this dataset to Supervised Fine-Tune (SFT) a much smaller, open-weights model, like **Llama-3-8B**. We use QLoRA to keep training costs under \$100.
*   **Self-Hosting:** We deploy this fine-tuned Llama-3-8B model on our own AWS infrastructure using an optimized inference engine like **vLLM** with PagedAttention to maximize throughput.
*   *Result:* Instead of paying per token, we pay a flat hourly rate for the EC2 GPU instance. Because the model is heavily fine-tuned for this one specific task, it performs just as well as GPT-4o, but costs literally 1/100th of the price per query at scale.

## 6. System Evaluation
How do we prove to the PMs that the cheaper Llama 3 model is still good?
*   **LLM-as-a-Judge:** We run a daily pipeline where 1% of the Llama 3 summaries are routed to GPT-4o. GPT-4o grades the Llama 3 summary on a scale of 1-5 for accuracy and fluency. We create a Grafana dashboard showing that the "Quality Score" remained stable at 4.8/5 even after the transition.

## Interview Strategy
"My approach to cost optimization follows a funnel. First, I minimize the denominator by aggressively stripping useless context from the prompts using Python. Second, I implement an **LLM Router** to divert simple tasks to cheaper models like Haiku. Finally, if the volume justifies the engineering time, I leverage **Knowledge Distillation**, using the expensive proprietary model to generate a dataset to fine-tune a small, cheap open-source model (like Llama 3) that we self-host via vLLM."